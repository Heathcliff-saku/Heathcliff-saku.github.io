<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<!-- saved from url=(0028)https://mayuelala.github.io/ -->
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
		<link rel="shortcut icon" href="files/icon.png">
		<meta name="keywords" content="Shouwei Ruan, Ruan Shouwei, 阮受炜, BUAA, Beihang University, ruan shouwei, XDU, Xidian University, Tsinghua University"> 
		<meta name="description" content="Shouwei Ruan&#39;s Homepage">
		<meta name="google-site-verification" content="Qtc44kk97LToDsCGgmBubRkXSP4byJ_WPM4BF3aglEM">
		<link rel="stylesheet" href="./files/jemdoc.css" type="text/css">
		<title>Shouwei Ruan (阮受炜)</title>
		<style>
			table {
				border-collapse: collapse;
			}
			.smaller-image {
			width: 20%;
			}
			.container {
				position: relative;
				display: inline-block;   
			}

			.image {
				display: block;
				border-radius: 10px;
				max-width: 100%;
			}
			.video-label {
				position: absolute;
				top: 5px;
				left: 5px;
				background-color: #ea3323;
				color: white;
				padding: 0px 10px;
				border-radius: 3px;
				font-weight: bold;
				box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19);
				width: 40px;
				height: 20px;
				display: flex;
				justify-content: center;
				align-items: center;
				font-size:small;
			}
			.research-hightlight {
				display: grid;
				grid-template-columns: repeat(2, 1fr); /* 5列 */
				/* grid-template-rows: repeat(4, 1fr);  */
				gap: 10px; /* 网格项之间的间隙 */
			}
			.arxiv-label {
				position: absolute;
				top: 10px;
				left: -5px;
				background-color: #a624a6;
				color: white;
				padding: 0px 10px;
				border-radius: 3px;
				font-weight: bold;
				box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19);
				width: 40px;
				height: 20px;
				display: flex;
				justify-content: center;
				align-items: center;
				font-size:x-small;
			}
		</style>
		<script type="text/javascript" src="./files/jquery-1.12.4.min.js"></script>
		
</head>
<body data-new-gr-c-s-check-loaded="14.1163.0" data-gr-ext-installed="">
<div id="layout-content" style="margin-top:25px">
 <a href="https://github.com/Heathcliff-saku" class="github-corner"><svg width="80" height="80" viewBox="0 0 250 250" style="fill: #527bbd; color:#fff; position: absolute; top: 0; border: 0; right: 0;"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

<table>
	<tbody>
		<tr>
			<td width="720">
				<div id="toptitle">					
					<h1> Shouwei Ruan (阮受炜) </h1>
					<h1></h1>
					<div style="display: flex; align-items: center; gap: 20px;">
						<img src="./files/sign.jpg" alt="" style="width: 25%; height: auto;">
						<img src="./files/institute/buaa-rose.png" alt="" style="width: 25%; height: auto;">
					</div>
				</div>
				<h3 style="margin-top: -20px;">PhD Candidate @ Institute of AI, BUAA </h3>
				<h3 style="margin-top: -10px;">AI Security & Embodied-AI Researcher @ 🇨🇳</h3>
				<p  style="margin-top: 20px;">
					<!-- Computer Science and Technology,
					<a href="https://csse.szu.edu.cn/" target="_blank">CSSE</a>,<br>
					<a href="https://en.szu.edu.cn/" target="_blank">Shenzhen University (SZU)</a><br> -->
					
					<!-- <br>
					<p></p> -->
					<a href="mailto:shouweiruan@gmail.com"" target="_blank"><img src="./files/icon/email.png" height="32px" style="margin-bottom:-4px"></a>&nbsp;
					<a href="https://scholar.google.com/citations?user=1pggtuUAAAAJ&hl=zh-CN&oi=ao" target="_blank"><img src="./files/icon/google_scholar.png" height="30px" style="margin-bottom:-3px"></a>&nbsp;
					<a href="https://github.com/Heathcliff-saku" target="_blank"><img src="./files/icon/github_s.jpg" height="30px" style="margin-bottom:-3px"></a>&nbsp;
					<a href="./files/wechat.jpeg" target="_blank"><img src="./files/icon/wechat.png" height="30px" style="margin-bottom:-3px"></a>&nbsp;
					<a href="https://www.zhihu.com/people/ruan-mou-ren-26" target="_blank"><img src="./files/icon/zhihu.png" height="30px" style="margin-bottom:-3px"></a>&nbsp; 
					<a href="https://www.xiaohongshu.com/user/profile/601961b40000000001002a7c" target="_blank"><img src="./files/icon/xiaohongshu.png" height="31px" style="margin-bottom:-4px"></a>&nbsp;
					<a href="./files/阮受炜简历2024-07.pdf" target="_blank">[中文简历-2024.07]</a>
					<!-- <a href="https://www.xiaohongshu.com/user/profile/618fe930000000001000afe7"><img src="./files/ins.png" height="30px" style="margin-bottom:-4px"></a>&nbsp; -->
					<!-- <a href="https://www.xiaohongshu.com/user/profile/618fe930000000001000afe7"><img src="./files/google_scholar_large.png" height="30px" style="margin-bottom:-4px"></a>&nbsp; -->
					<!-- <a href="https://drive.google.com/file/d/1PKS9U8kX0m4w7jEmpDMK8S3hFe5x3c0L/view">[Resume]</a>&nbsp;<a href="https://drive.google.com/file/d/1PKS9U8kX0m4w7jEmpDMK8S3hFe5x3c0L/view">[中文简历]</a> -->
					<p></p>
					<!-- <a href="mailto:chentianxing2002@gmail.com" target="_blank">Email</a> / <a href="https://scholar.google.com/citations?hl=en&user=pvS8MH8AAAAJ&view_op=list_works&gmla=AOAOcb35IyZHtGmmYcpnDrJFmcsHLBXzjnq0ChbL0CXg4-PjM5UXRspLHuzXI4jgPc077WejF7RSsLUULIZ5ugIxcns6FURGdnTSpPi9JhAeKfhLVXsAIauozmPDdYzcku8VruOeRoapXM7nhkTlaNQ&iaan=Tianxing+Chen", target="_blank">Google Scholar</a> / <a href="https://github.com/tianxingchen" target="_blank">Github</a> / <a href="./files/my_wechat.jpg" target="_blank">微信</a> / <a>Resume</a><br><br> -->
					<a href="https://hits.seeyoufarm.com"><img src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fheathcliff-saku.github.io&count_bg=%233FDBD2&title_bg=%233D62C7&icon=googlepodcasts.svg&icon_color=%23E9F742&title=Page+Viewers&edge_flat=false"/></a>
					<!-- <img alt="X (formerly Twitter) Follow" src="https://img.shields.io/twitter/follow/MarioChan2002"> -->
					<img alt="GitHub User's stars" src="https://img.shields.io/github/stars/Heathcliff-saku">
					<img alt="GitHub followers" src="https://img.shields.io/github/followers/Heathcliff-saku">
					<!-- <a href='https://scholar.google.com/citations?user=1pggtuUAAAAJ&hl=zh-CN&oi=ao'><img src="https://img.shields.io/endpoint?logo=Google%20Scholar&url=https%3A%2F%2Fcdn.jsdelivr.net%2Fgh%2FRayeRen%2Frayeren.github.io@google-scholar-stats%2Fgs_data_shieldsio.json&labelColor=f6f6f6&color=9cf&style=flat&label=citations"></a> -->
				</p>
			</td>
			<td>
				<!-- <img src="./files/avatar12.jpg" border="0" width="250"><br> -->
				<img src="./files/avatar/shouwei_img.jpeg" border="0" width="250"><br>
				<!-- <a href="https://hits.seeyoufarm.com"><img src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fchen-tianxing.github.io&count_bg=%2321E0D7&title_bg=%23245DD3&icon=github.svg&icon_color=%23E7E7E7&title=Visits+%28today+%2F+total%29&edge_flat=false"/></a> -->
				<!-- <a href="https://hits.seeyoufarm.com"><img src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2Ftianxingchen&count_bg=%238710FF&title_bg=%23E140D1&icon=&icon_color=%23E7E7E7&title=Github+Viewers&edge_flat=false"/></a><br>
				<a href="https://hits.seeyoufarm.com"><img src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2Ftianxingchen&count_bg=%23D5FF10&title_bg=%2340E1AB&icon=&icon_color=%23E7E7E7&title=Google+Scholar+Viewers&edge_flat=false"/></a> -->
				<!-- <img src="https://img.shields.io/github/stars/tianxingchen?style=social" alt="GitHub stars"> -->
				<!-- <br> -->
			</td>
		</tr><tr>
	</tr></tbody>
</table>


<h2 style="margin-top: -10px;">Biography</h2>
<div style="font-size: 15px">

Hi! I am <strong>Shouwei Ruan (阮受炜)</strong>, currently a 3th-year Ph.D. student in the Institute of Artificial Intelligence, Beihang University (BUAA). 
I am the member of <a href="https://rose-vision.github.io/" target="_blank">ROSE Vision Lab</a>, advised by <a href="https://sites.google.com/site/xingxingwei1988/"> Prof. Xingxing Wei (韦星星｜国家级青年人才) </a>.
I am currently a <u>visiting student</u> at the <a href="https://ml.cs.tsinghua.edu.cn/" target="_blank">TSAIL@Tsinghua University</a>, under the joint supervision of <a href="https://www.suhangss.me/"> Prof. Hang Su (苏航) </a> and <a href="https://ml.cs.tsinghua.edu.cn/~yinpeng/"> A/Prof. Yinpeng Dong (董胤蓬) </a>.<br><br>

I received my B.E. degree from Xidian University (XDU), where I was awarded the <strong>National Scholarship</strong> in 2021. I was also honored with the titles of <strong>Outstanding Graduate</strong> and <strong>Innovation Model (西电创新楷模)</strong> of Xidian University. In 2021, I worked as a research intern at the Digital Team of <a href="https://www.dell.com/zh-cn" target="_blank">Dell Technologies Co. (戴尔科技中国)</a>, under the mentorship of SSE. Xuejin Liang. In 2022, I served as a research intern at <a href="https://www.realai.ai/" target="_blank">RealAI (瑞莱智慧)</a>, mentored by A/Prof. Yinpeng Dong.<br><br>

My current research interests focus on the applications of vision-language models and their robustness in <b>spatial perception</b>, including 3D relationships and viewpoint transformations. I am also interested in the <b>trustworthiness and security</b> of Embodied AI systems, as well as research on vision-language navigation tasks.<br><br>
<i style="color: red; display: inline;">Feel free to contact me by email if you are interested in discussing or collaborating with me.</i>
</div>

<h2>News</h2>

<div style="height: 100px; overflow: auto; font-size: small;">
<ul>
	<li>
		[01/2025] Our work on the robustness of face recognition: “Distributionally Location-Aware Transferable Adversarial Patches for Facial Images”, 
		has been accepted by <i style="color: rgb(111, 0, 255); display: inline;">IEEE TPAMI</i> !
	</li>
	<li>
		[07/2024] Our paper "Omniview-Tuning: Boosting Viewpoint Invariance of Vision-Language Pre-training Models" has been selected for 
		an <i style="color: red; display: inline;">Oral presentation</i> at <i style="color: rgb(111, 0, 255); display: inline;">ECCV 2024</i>. Please see more details in our <a href="https://omniview-tuning.github.io/" target="_blank">project page</a>.
	</li>
</ul>
</div>
<div style="margin-top: 5px; font-size: small; margin-bottom: 0px;">⬆ scrollable</div>



	<h2>Education</h2>
	<!-- <p></p><p></p>&nbsp;&nbsp;&nbsp; -->
	<table id="tbPublications" width="100%" style="margin-top: 0px">
		<table>
			<tr>
				<td width="320">
					<img src="./files/institute/buaa.png" width="250px" style="margin-left: 40px">
				</td>				
				<td>
					<p><b>ROSE Vision Lab, Beihang University (BUAA), Beijing, China</b></p>
					<div style="font-size: 12px">Ph.D. Candidate in Artificial Intelligence (Computer Science and Technology), advised by Prof. Xingxing Wei.</div>
					<div style="font-size: 12px">Sep. 2022 - Jun. 2027 (expected)<div>
				</td>
			</tr>
		</tbody>
	</table>

	<table id="tbPublications" width="100%" style="margin-top: 0px">
		<table>
			<tr>
				<td width="320">
					<img src="./files/institute/xdu.jpg" width="210px" style="margin-left: 30px">
				</td>				
				<td>
					<p><b>Xidian University (XDU), Xi'an, China</b></p>
					<div style="font-size: 12px">Bachelor's in Intelligent Science and Technology, Sep. 2018 - Jun. 2022.</div>
					<div style="font-size: 12px">GPA: 3.8/4.0; Overall Ranking: 2/310<div>
				</td>
			</tr>
		</tbody>
	</table>


<!-- <h2>Research Group</h2>
<table id="tbPublications" width="100%">
	<tbody>
		<tr>
			<td width="306">
			<img src="./files/AARG.png" width="150px" style="margin-left: 75px;">
			</td>				
			<p><b>RAgent Research Group (RARG)</b></p>
			<td>
			<p>Since May. 2024, as founder.</p>
			[<a href="https://github.com/AtomAgent-Research-Group-AARG">Github</a>]
			<p>We are an academic group focusing on <b>Embodied AI</b> and <b>Agent</b> research.</p>
			<em>Feel free to contact me if you are interested in joining us !</em>
			</td>
		</tr>
	</tbody>
</table> -->

<!-- <iframe src="//player.bilibili.com/player.html?isOutside=true&aid=1704962157&bvid=BV1BT421S7fM&cid=1553434132&p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"></iframe> -->

<h2> Publications | <a href="https://scholar.google.com/citations?user=1pggtuUAAAAJ&hl=zh-CN&oi=ao" target="_blank">[Google Scholar]</a></h2>
<div style="height: 10px;"></div>
<div style="text-align: center; position: relative; margin-bottom: 20px; margin-top: 0px;">
	<span style="position: absolute; top: 50%; left: 0; right: 0; transform: translateY(-50%); background-color: rgba(128, 128, 128, 0.3); padding: 0 10px;">
		<b>Recent Research Highlights</b>
	</span>
	<!-- <hr style="border: 0; height: 1px; background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.5), rgba(0, 0, 0, 0)); width: 100%;"> -->
  </div>
  <div class="research-hightlight" style="display: flex; justify-content: center; align-items: center; gap: 20px;">
	<div style="text-align: center;">
		<img src="./files/work/poster-omniviewtuning.png" alt="Omniview-Tuning" style="height: 320px; width: auto; display: block; margin: 0 auto;">
		<div style="margin-top: 5px; font-size: small;">
			Omniview-Tuning<i>, ECCV2024 <i style="color: red; display: inline;">Oral</i> 🎖️ </i>
		</div>
	</div>
	<div style="text-align: center;">
		<img src="./files/work/poster-advdreamer.png" alt="AdvDreamer" style="height: 320px; width: auto; display: block; margin: 0 auto;">
		<div style="margin-top: 5px; font-size: small;">
			AdvDreamer, <i>arXiv 2024</i>
		</div>
	</div>
</div>

<div style="text-align: center; position: relative; margin-bottom: 50px; margin-top: 20px;">
	<span style="position: absolute; top: 50%; left: 0; right: 0; transform: translateY(-50%); background-color: rgba(128, 128, 128, 0.3); padding: 0 10px;">
		<b>Full List</b>
	</span>
	<!-- <hr style="border: 0; height: 1px; background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.5), rgba(0, 0, 0, 0)); width: 100%;"> -->
  </div>

<table id="tbPublications" width="100%" cellspacing='0' style="background-color: #ffffdf;">
	<tbody>
	<!--########################-->
	<tr style="width: 100%;">
		<td width="20%" style="padding: 10px">
			<div class="container">
				<img src="./files/work/2025-dopatch.jpg" width="200px" style="box-shadow: 4px 4px 4px #888888; margin-left: 10px;">
				<!-- <div class="arxiv-label" style="margin-left: 10px; width: 80px;">Under Review</div> -->
			</div>
		</td>				
		<td style="padding: 10px">
			<!-- <div style="height: 10px"></div> -->
			<b style="font-size: 15px; color: #7c45c3;">Distributionally Location-Aware Transferable Adversarial Patches for Facial Images</b>
			<div style="height: 5px;"></div>
			<div style="font-size: 12px">Xingxing Wei(advisor)<sup>†</sup>, <b><u>Shouwei Ruan</u></b>, Yinpeng Dong, Hang Su, Xiaochun Cao<sup>†</sup></div>
			<div style="height: 5px;"></div>
			<div style="font-size: 12px">IEEE Transactions on Pattern Analysis and Machine Intelligence (<i style="color: rgb(255, 85, 0); display: inline;"><b>TPAMI</b>, CCF-A</i>), 2025</div>
			<div style="height: 5px;"></div>
			<div style="font-size: 12px"><a href="https://ieeexplore.ieee.org/abstract/document/10829780" target="_blank">Paper</a> / <a href="https://arxiv.org/pdf/2306.16131" target="_blank">arXiv (early-version)</a> / <a href="https://github.com/Heathcliff-saku/DOPatch" target="_blank">Code</a> </div>
			<div style="height: 5px;"></div>
			<div style="font-size: 12px; color: gray;">By leveraging the distribution transferability of adversarial patches in placement locations, we propose distribution modeling to enhance the performance of location-aware patches. Through a distribution mapping network, we learn the adversarial position distribution of images on the surrogate model and further transfer the distribution prior to models in black-box settings, enabling efficient query-based patch attacks.</div>
			<!-- <div style="height: 10px"></div> -->
		</td>
	</tr>
	</tbody>
</table>
<table id="tbPublications" width="100%" cellspacing='0' style="background-color: #ffffdf;">
	<tbody>
	<!--########################-->
	<tr style="width: 100%;">
		<td width="20%" style="padding: 10px">
			<div class="container">
				<img src="./files/work/2024-advdreamer.png" width="200px" style="box-shadow: 4px 4px 4px #888888; margin-left: 10px;">
				<!-- <div class="arxiv-label" style="margin-left: 10px; width: 80px;">Under Review</div> -->
			</div>
		</td>				
		<td style="padding: 10px">
			<!-- <div style="height: 10px"></div> -->
			<b style="font-size: 15px; color: #7c45c3;">🔥 AdvDreamer Unveils: Are Vision-Language Models Truly Ready for Real-World 3D Variations?</b>
			<div style="height: 5px;"></div>
			<div style="font-size: 12px"><b><u>Shouwei Ruan</u></b><sup>*</sup>, Hanqin Liu<sup>*</sup>, Yao Huang, Xiaoqi Wang, Caixin Kang, Hang Su, Yinpeng Dong<sup>†</sup>, Xingxing Wei<sup>†</sup></div>
			<div style="height: 5px;"></div>
			<div style="font-size: 12px">Under Review, 2025</div>
			<div style="height: 5px;"></div>
			<div style="font-size: 12px"><a href="https://arxiv.org/pdf/2412.03002" target="_blank">arXiv</a> / <a target="_blank">Code (Coming Soon)</a></div>
			<div style="height: 5px;"></div>
			<div style="font-size: 12px; color: gray;">To systematically evaluate VLMs' robustness to real-world
				3D variations, we propose AdvDreamer, the first framework that generates physically reproducible adversarial 3D
				transformation (Adv-3DT) samples from single-view images. Leveraging AdvDreamer, we establish MM3DTBench, 
				the first VQA dataset for benchmarking VLMs' 3D variations robustness. Extensive evaluations on representative VLMs with diverse architectures
				highlight that 3D variations in the real world may pose severe threats to model performance across various tasks.</div>
			<!-- <div style="height: 10px"></div> -->
		</td>
	</tr>
	</tbody>
</table>
<table id="tbPublications" width="100%" cellspacing='0'>
	<tbody>
	<!--########################-->
	<tr style="width: 100%;">
		<td width="20%" style="padding: 10px">
			<div class="container">
				<img src="./files/work/2024-oodface.png" width="200px" style="box-shadow: 4px 4px 4px #888888; margin-left: 10px;;">
				<!-- <div class="arxiv-label" style="margin-left: 10px; width: 80px;">Under Review</div> -->
			</div>
		</td>				
		<td style="padding: 10px">
			<!-- <div style="height: 10px"></div> -->
			<b style="font-size: 15px; color: #456fc3;">OODFace: Benchmarking Robustness of Face Recognition under Common Corruptions and Appearance Variations</b>
			<div style="height: 5px;"></div>
			<div style="font-size: 12px">Caixin Kang, Yubo Chen, <b><u>Shouwei Ruan</u></b>, Shiji Zhao, Ruochen Zhang, Jiayi Wang, Shan Fu, Xingxing Wei<sup>†</sup></div>
			<div style="height: 5px;"></div>
			<div style="font-size: 12px">Under Review, 2025</div>
			<div style="height: 5px;"></div>
			<div style="font-size: 12px"><a href="https://arxiv.org/pdf/2412.02479" target="_blank">arxiv</a> / <a target="_blank">Code (Coming Soon)</a></div>
			<div style="height: 5px;"></div>
			<div style="font-size: 12px; color: gray;">In this paper, we introduce OODFace, which explores the OOD challenges faced by
				facial recognition models from two perspectives: common corruptions and appearance variations. We systematically
				design 30 OOD scenarios across 9 major categories tailored for facial recognition.</div>

			<!-- <div style="height: 10px"></div> -->
		</td>
	</tr>
	</tbody>
</table>
<table id="tbPublications" width="100%" cellspacing='0' style="background-color: #ffffdf;">
	<tbody>
	<!--########################-->
	<tr style="width: 100%;">
		<td width="20%" style="padding: 10px">
			<div class="container">
				<img src="./files/work/2024-omniviewtuning.png" width="200px" style="box-shadow: 4px 4px 4px #888888; margin-left: 10px;">
				<!-- <div class="arxiv-label" style="margin-left: 10px; width: 80px;">Under Review</div> -->
			</div>
		</td>				
		<td style="padding: 10px">
			<!-- <div style="height: 10px"></div> -->
			<b style="font-size: 15px; color: #7c45c3;">🔥 Omniview-Tuning: Boosting Viewpoint Invariance of VLP Models</b>
			<div style="height: 5px;"></div>
			<div style="font-size: 12px"><b><u>Shouwei Ruan</u></b>, Yinpeng Dong, Hanqing Liu, Yao Huang, Hang Su, Xingxing Wei<sup>†</sup></div>
			<div style="height: 5px;"></div>
			<div style="font-size: 12px">European Conference on Computer Vision (<i style="color: rgb(255, 85, 0); display: inline;"><b>ECCV Oral</b>, CCF-B</i>), Milan, Italy, 2024. </div>
			<div style="height: 5px;"></div>
			<div style="font-size: 12px"><a href="https://omniview-tuning.github.io/" target="_blank">Project page</a> / <a href="https://link.springer.com/chapter/10.1007/978-3-031-73347-5_18" target="_blank">Paper</a> / <a href="https://arxiv.org/pdf/2404.12139" target="_blank">arxiv</a> /
				<a href="https://github.com/Heathcliff-saku/Omniview_Tuning" target="_blank">Code</a> / <a href="https://huggingface.co/datasets/RSW233/MVCap-4M" target="_blank">Dataset</a> 
			/ <a href="https://hub.baai.ac.cn/paper/1ce347ac-9279-4031-aeb7-9d94003afe44" target="_blank">智源社区</a> / <a href="https://www.techbeat.net/talk-info?id=908" target="_blank">TechBeat Talk</a>
			</div>
			<div style="height: 5px;"></div>
			<div style="font-size: 12px; color: gray;">
				Vision-Language Pre-training (VLP) models' robustness under 3D viewpoint variations is still limited, which can
				hinder the development for real-world applications. This paper successfully addresses this concern while keeping VLPs' original performance by
				breaking through two primary obstacles: 1) the scarcity of training data
				and 2) the suboptimal fine-tuning paradigms. To combat data scarcity,
				we build the Multi-View Caption (MVCap) dataset — a comprehensive
				collection of over four million multi-view image-text pairs across more
				than 100K objects, providing more potential for VLP models to develop
				generalizable viewpoint-invariant representations. To address the limitations of existing paradigms in performance trade-offs and training efficiency, we design a novel fine-tuning framework named Omniview-Tuning
				(OVT). </div>
			<!-- <div style="height: 10px"></div> -->
		</td>
	</tr>
	</tbody>
</table>
<table id="tbPublications" width="100%" cellspacing='0'>
	<tbody>
	<!--########################-->
	<tr style="width: 100%;">
		<td width="20%" style="padding: 10px">
			<div class="container">
				<img src="./files/work/2024-diffender.png" width="200px" style="box-shadow: 4px 4px 4px #888888; margin-left: 10px;;">
				<!-- <div class="arxiv-label" style="margin-left: 10px; width: 80px;">Under Review</div> -->
			</div>
		</td>				
		<td style="padding: 10px">
			<!-- <div style="height: 10px"></div> -->
			<b style="font-size: 15px; color: #456fc3;">DIFFender: Diffusion-based Adversarial Defense Against Patch Attack</b>
			<div style="height: 5px;"></div>
			<div style="font-size: 12px">Caixin Kang, Yinpeng Dong, Zhengyi Wang, <b><u>Shouwei Ruan</u></b>, Hang Su<sup>†</sup>, Xingxing Wei<sup>†</sup></div>
			<div style="height: 5px;"></div>
			<div style="font-size: 12px">Conf. Version: European Conference on Computer Vision (<i style="color: rgb(255, 85, 0); display: inline;"><b>ECCV</b>, CCF-B</i>), Milan, Italy, 2024. </div>
			<div style="font-size: 12px">Journal Version: <a href="https://arxiv.org/pdf/2409.09406" target="_blank">[arxiv]</a>, Under Review, 2024 </div>
			<div style="height: 5px;"></div>
			<div style="font-size: 12px"><a href="https://link.springer.com/chapter/10.1007/978-3-031-72943-0_8" target="_blank">Paper</a> / <a href="https://arxiv.org/pdf/2306.09124" target="_blank">arxiv</a> /
				<a href="https://github.com/kkkcx/DIFFender" target="_blank">Code</a>
			</div>
			<div style="height: 5px;"></div>
			<div style="font-size: 12px; color: gray;"> This paper introduces DIFFender, a novel DIFfusion-based DeFender
				framework that leverages the power of a text-guided diffusion model to counter adversarial patch attacks. At the core of our approach is
				the discovery of the Adversarial Anomaly Perception (AAP) phenomenon, which enables the diffusion model to accurately detect and
				locate adversarial patches by analyzing distributional anomalies.</div>

			<!-- <div style="height: 10px"></div> -->
		</td>
	</tr>
	</tbody>
</table>
<table id="tbPublications" width="100%" cellspacing='0'>
	<tbody>
	<!--########################-->
	<tr style="width: 100%;">
		<td width="20%" style="padding: 10px">
			<div class="container">
				<img src="./files/work/2024-tt3d.png" width="200px" style="box-shadow: 4px 4px 4px #888888; margin-left: 10px;;">
				<!-- <div class="arxiv-label" style="margin-left: 10px; width: 80px;">Under Review</div> -->
			</div>
		</td>				
		<td style="padding: 10px">
			<!-- <div style="height: 10px"></div> -->
			<b style="font-size: 15px; color: #456fc3;">Towards Transferable Targeted 3D Adversarial Attack in the Physical World</b>
			<div style="height: 5px;"></div>
			<div style="font-size: 12px">Yao Huang, Yinpeng Dong<sup>†</sup>,  <b><u>Shouwei Ruan</u></b>, Xiao Yang, Hang Su, Xingxing Wei<sup>†</sup></div>
			<div style="height: 5px;"></div>
			<div style="font-size: 12px">IEEE Conference on Computer Vision and Pattern Recognition (<i style="color: rgb(255, 85, 0); display: inline;"><b>CVPR</b>, CCF-A</i>), Seattle, USA, 2024. </div>
			<div style="height: 5px;"></div>
			<div style="font-size: 12px"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Huang_Towards_Transferable_Targeted_3D_Adversarial_Attack_in_the_Physical_World_CVPR_2024_paper.pdf" target="_blank">Paper</a> / <a href="https://arxiv.org/pdf/2312.09558" target="_blank">arxiv</a> /
				<a href="https://github.com/Aries-iai/TT3D" target="_blank">Code</a>
			</div>
			<div style="height: 5px;"></div>
			<div style="font-size: 12px; color: gray;"> In this paper, We design a novel framework named TT3D that could rapidly reconstruct from few multi-view images into Transferable Targeted 3D textured meshes.</div>

			<!-- <div style="height: 10px"></div> -->
		</td>
	</tr>
	</tbody>
</table>
<table id="tbPublications" width="100%" cellspacing='0'>
	<tbody>
	<!--########################-->
	<tr style="width: 100%;">
		<td width="20%" style="padding: 10px">
			<div class="container">
				<img src="./files/work/2024-embodied_attack.jpg" width="200px" style="box-shadow: 4px 4px 4px #888888; margin-left: 10px;;">
				<!-- <div class="arxiv-label" style="margin-left: 10px; width: 80px;">Under Review</div> -->
			</div>
		</td>				
		<td style="padding: 10px">
			<!-- <div style="height: 10px"></div> -->
			<b style="font-size: 15px; color: #456fc3;">Exploring the Robustness of Decision-Level Through Adversarial Attacks on LLM-Based Embodied Models</b>
			<div style="height: 5px;"></div>
			<div style="font-size: 12px">Liu Shuyuan<sup>*</sup>, Jiawei Chen<sup>*</sup>, <b><u>Shouwei Ruan</u></b>, Hang Su, Zhaoxia Yin<sup>†</sup></div>
			<div style="height: 5px;"></div>
			<div style="font-size: 12px">ACM International Conference on Multimedia (<i style="color: rgb(255, 85, 0); display: inline;"><b>ACM MM</b>, CCF-A</i>), Melbourne, Australia, 2024. </div>
			<div style="height: 5px;"></div>
			<div style="font-size: 12px"><a href="https://arxiv.org/pdf/2405.19802" target="_blank">Paper</a> / <a href="https://arxiv.org/pdf/2405.19802" target="_blank">arxiv</a> /
				<a target="_blank">Code (Coming Soon)</a>
			</div>
			<div style="height: 5px;"></div>
			<div style="font-size: 12px; color: gray;"> We observe a notable absence of multi-modal datasets essential for comprehensively evaluating the robustness of LLM-based embodied models. Consequently, we construct the Embodied Intelligent Robot Attack Dataset (EIRAD), tailored specifically for robustness evaluation. Additionally, two attack strategies are devised, including untargeted attacks and targeted attacks, to effectively simulate a range of diverse attack scenarios.</div>

			<!-- <div style="height: 10px"></div> -->
		</td>
	</tr>
	</tbody>
</table>

<table id="tbPublications" width="100%" cellspacing='0' style="background-color: #ffffdf;">
	<tbody>
	<!--########################-->
	<tr style="width: 100%;">
		<td width="20%" style="padding: 10px">
			<div class="container">
				<img src="./files/work/2023-viat.jpg" width="200px" style="box-shadow: 4px 4px 4px #888888; margin-left: 10px;">
				<!-- <div class="arxiv-label" style="margin-left: 10px; width: 80px;">Under Review</div> -->
			</div>
		</td>				
		<td style="padding: 10px">
			<!-- <div style="height: 10px"></div> -->
			<b style="font-size: 15px; color: #7c45c3;">🔥 Towards Viewpoint-Invariant Visual Recognition via Adversarial Training</b>
			<div style="height: 5px;"></div>
			<div style="font-size: 12px"><b><u>Shouwei Ruan</u></b>, Yinpeng Dong, Hang Su<sup>†</sup>, Jianteng Peng, Ning Chen, Xingxing Wei<sup>†</sup></div>
			<div style="height: 5px;"></div>
			<div style="font-size: 12px">Conf. Version: International Conference on Computer Vision (<i style="color: rgb(255, 85, 0); display: inline;"><b>ICCV</b>, CCF-A</i>), Paris, France, 2023. </div>
			<div style="font-size: 12px">Journal Version: <a href="https://arxiv.org/pdf/2307.11528" target="_blank">[arxiv]</a>, Under Review, 2024 </div>
			<div style="height: 5px;"></div>
			<div style="font-size: 12px"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ruan_Towards_Viewpoint-Invariant_Visual_Recognition_via_Adversarial_Training_ICCV_2023_paper.pdf" target="_blank">Paper</a> / <a href="https://arxiv.org/pdf/2307.10235" target="_blank">arxiv</a> /
				<a href="https://github.com/Heathcliff-saku/VIAT" target="_blank">Code</a> / <a href="https://drive.google.com/file/d/1_A4ePjOhlJahJpy8T2dgWZLoigKmqqSd/view" target="_blank">Dataset (IM3D)</a> 
			/ <a href="https://drive.google.com/file/d/1oxrWl4mRa_mEr-ByCMhyRWaQG8Wribo7/view" target="_blank">Dataset (IN-V+)</a> / <a href="https://www.techbeat.net/talk-info?id=908" target="_blank">TechBeat Talk</a>
			</div>
			<div style="height: 5px;"></div>
			<div style="font-size: 12px; color: gray;">
				we propose Viewpoint-Invariant Adversarial Training (VIAT) to improve viewpoint robustness of common image classifiers.
				Moreover, we construct a new out-ofdistribution (OOD) benchmark—ImageNet-V+, containing
				nearly 100k images from the adversarial viewpoints found
				by GMVFool. </div>
			<!-- <div style="height: 10px"></div> -->
		</td>
	</tr>
	</tbody>
</table>

<table id="tbPublications" width="100%" cellspacing='0' style="background-color: #ffffdf;">
	<tbody>
	<!--########################-->
	<tr style="width: 100%;">
		<td width="20%" style="padding: 10px">
			<div class="container">
				<img src="./files/work/2022-viewfool.jpg" width="200px" style="box-shadow: 4px 4px 4px #888888; margin-left: 10px;">
				<!-- <div class="arxiv-label" style="margin-left: 10px; width: 80px;">Under Review</div> -->
			</div>
		</td>				
		<td style="padding: 10px">
			<!-- <div style="height: 10px"></div> -->
			<b style="font-size: 15px; color: #7c45c3;">🔥 Viewfool: Evaluating the Robustness of Visual Recognition to Adversarial Viewpoints</b>
			<div style="height: 5px;"></div>
			<div style="font-size: 12px">Yinpeng Dong, <b><u>Shouwei Ruan</u></b>, Hang Su, Caixin Kang, Xingxing Wei, Jun Zhu<sup>†</sup></div>
			<div style="height: 5px;"></div>
			<div style="font-size: 12px">Advances in Neural Information Processing Systems (<i style="color: rgb(255, 85, 0); display: inline;"><b>NeurIPS</b>, CCF-A</i>), New Orleans, USA, 2022. </div>
			<div style="height: 5px;"></div>
			<div style="font-size: 12px"><a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/eee7ae5cf0c4356c2aeca400771791aa-Paper-Conference.pdf" target="_blank">Paper</a> / <a href="https://arxiv.org/pdf/2210.03895" target="_blank">arxiv</a> /
				<a href="https://github.com/Heathcliff-saku/ViewFool_" target="_blank">Code</a> / <a href="https://drive.google.com/file/d/1i6QBGzERxo2br33WwIGi6GyUZasqCWp-/view" target="_blank">Dataset (IN-V)</a>  / <a href="https://www.techbeat.net/talk-info?id=908" target="_blank">TechBeat Talk</a>
			</div>
			<div style="height: 5px;"></div>
			<div style="font-size: 12px; color: gray;">
				In this paper, we propose a novel method called ViewFool to find adversarial viewpoints
				that mislead visual recognition models. By encoding real-world objects as neural radiance fields (NeRF), ViewFool characterizes a distribution of diverse adversarial
				viewpoints under an entropic regularizer, which helps to handle the fluctuations of the real camera pose and mitigate the reality gap between the real objects and their neural representations.  </div>
			<!-- <div style="height: 10px"></div> -->
		</td>
	</tr>
	</tbody>
</table>


<i style="font-size: 12px"><b>*</b> Equal contribution. <b>†</b> Corresponding author.</i>
<!--
</div>
-->

<h2>Research &amp; Visiting Experience</h2>
<div style="height: 10px"></div>
<table id="tbPublications" width="100%">
	<tbody>
	<tr>
		<td width="320">
		<img src="./files/institute/thu-tsail.png" width="250px" style="margin-left: 23px">
		</td>				
		<td>
			<p><b>TSAIL Group, Tsinghua University (THU), Beijing, China</b></p>
		
			<div style="font-size: 12px">Visiting PhD. Student (2023 - Present) @ TSAIL<div>
			<p style="font-size: 12px"> Supervised by Prof. Hang Su and 
				closely working with <a href="https://scholar.google.com/citations?user=6urFg8kAAAAJ&hl=en"> Songming Liu </a></p>
		</td>
	</tr>
	</tbody>
</table>

<table id="tbPublications" width="100%">
	<tbody>
	<tr>
		<td width="320">
		<img src="./files/institute/realai.png" width="200px" style="margin-left: 38px">
		</td>				
		<td>
			<p><b>Department of AI Security, RealAI (瑞莱智慧), Beijing, China</b></p>
			<div style="font-size: 12px">Research intern in AI Security (Mar.2022 - Aug.2022)<div>
			<p style="font-size: 12px">Supervised by Prof. Hang Su and A/Prof. Yinpeng Dong</p>
		</td>
	</tr>
	</tbody>
</table>

<table id="tbPublications" width="100%">
	<tbody>
	<tr>
		<td width="320">
		<img src="./files/institute/dell.png" width="200px" style="margin-left: 38px">
		</td>				
		<td>
			<p><b>Digital Team, Dell Technologies Co. (戴尔科技中国), Xiamen, China</b></p>
			<div style="font-size: 12px">Research intern in Deep Learning (Jul.2021 - Sep.2021)<div>
			<p style="font-size: 12px">Under the mentorship of SSE. Xuejin Liang (梁学锦｜资深软件工程师)</p>
		</td>
	</tr>
	</tbody>
</table>

<table id="tbPublications" width="100%">
	<tbody>
	<tr>
		<td width="320">
		<img src="./files/institute/hanyang.png" width="200px" style="margin-left: 38px">
		</td>				
		<td>
			<p><b>College of Engineering, Hanyang University (韩国汉阳大学), Seoul, South Korea</b></p>
			<div style="font-size: 12px">Visiting Student in Urban Environment and Universal Design (Jul.2019 - Sep.2019)<div>
			<p style="font-size: 12px">Under the mentorship of Prof. Yoojin Lee </a></p>
		</td>
	</tr>
	</tbody>
</table>

<h2>Academic Services</h2>
- Reviewer:
<div style="height: 100px; overflow: auto; font-size: 15px;">
	<ul>
		<li> International Journal of Computer Vision (IJCV).</li>
		<div style="margin-top: -10px;"></div>
		<li> IEEE Transactions on Multimedia (IEEE TMM).</li>
		<div style="margin-top: -10px;"></div>
		<li> IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR2025).</td></tr>
			<div style="margin-top: -10px;"></div>
		<li> International Conference on Learning Representations (ICLR2025).</li>
	</ul>
</div>
<div style="margin-top: 5px; font-size: small; margin-bottom: 0px;">⬆ scrollable</div>
<div style="margin-top: 10px;"></div>
<p>- Mobile researchers of QiYuan Lab (启元实验室), Beijing, China.</p>
<div style="margin-top: -10px;"></div>
<p>- Contracted Mentors of DeepShare (深度之眼), Shanghai, China.</p>
<div style="margin-top: -10px;"></div>
<p>- Teaching Assistants / Courses: </p>
<div style="height: 160px; overflow: auto; font-size: 15px;">
	<ul>
		<li> </p>Multi-modal Large Language Model: Basic Principles and Applications. (多模态大模型: 基础原理与前沿应用).</p>
			<p> Graduate Course on Pattern Recognition, BUAA, 2023. <a href="./files/mllm-course.pdf" target="_blank">[PDF Slides] </p> </a></li>
		<div style="margin-top: -10px;"></div>
		<li> </p>Large Language Models.	(大语言模型技术介绍).</p>
			<p> Graduate Course on Pattern Recognition, BUAA, 2024. <a href="./files/Lesson8_LLM.pdf" target="_blank">[PDF Slides] </p> </a></li>
		<div style="margin-top: -10px;"></div>
		<li> </p>Adversarial Attack Methods. (对抗攻击方法).</p>
			<p> Graduate Course on AI Scurity and Ethics, BUAA, 203. <a href="./files/attack-course.pdf" target="_blank">[PDF Slides] </p> </a></li>
	</ul>
</div>
<div style="margin-top: 5px; font-size: small; margin-bottom: 0px;">⬆ scrollable</div>

<h2>Talks & Reports</h2>

[10/2024], <a href="https://www.techbeat.net/talk-info?id=908" target="_blank"> 《Talk｜北京航空航天大学阮受炜：探索视觉感知的3D视角鲁棒性》</a>@ Techbeat<br>
where I shared our works on viewpoint robustness and invariance. <br>
<div style="margin-top: 10px;"></div>
<iframe width="560" height="315" src="https://www.youtube.com/embed/480mwUJv4hA?si=XHvtE-ysEZKRLBNK" 
title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
<div style="height: 4px"></div>

[05/2022], <a href="https://mp.weixin.qq.com/s/ggodjwGUCGbFLGOgKVtRQg" target="_blank"> 《国奖风采展｜阮受炜：执着求索，破而后立》</a>@ XDU<br>

[11/2021], <a href="https://mp.weixin.qq.com/s/3tjck75nxyMkH84eoAIBrA" target="_blank"> 《润物耕心|第二十五期回顾：以我之肩，载你之梦——竹园3号书院科研竞赛学长经验分享会》</a>@ XDU<br>


<h2>Awards & Honors (Selected)</h2>
<p></p>

<table style="border-spacing:2px">
	<tr></tr>
	<td width="280px">
			<img src="./files/award/web+.png" width="200px" style="box-shadow: 4px 4px 8px #888; margin-left: 32px; margin-top: 0px;">
	</td>				
	<td style="font-size: 15px;">
		<b>2021, China College Students' 'Internet+'Innovation and Entrepreneurship Competition </b>
		<p><b>中国国际“互联网+”大学生创新创业大赛</b></p>
		<p><i style="color: red; display: inline;"><b>National Gold Award, 6th Place in the Main Track Grand Final </b></i>🏆</p>
		<div style="font-size: 15px">With Ning Ji, et.al.</div>
	</td>
</table>

<table style="border-spacing:2px">
	<tr></tr>
	<td width="280px">
			<img src="./files/award/dell.png" width="200px" style="box-shadow: 4px 4px 8px #888; margin-left: 32px; margin-top: 0px;">
	</td>				
	<td style="font-size: 15px;">
		<b>2021, Intern Program Final Presentation @ Dell Technology </b>
		<p><b>戴尔科技实习生项目</b></p>
		<p><i style="color: red; display: inline;"><b>The Third Place </b></i>🥉</p>
		<div style="font-size: 15px">With Chengke Fan. Supervised by SSE. Xuejin Liang</div>
	</td>
</table>
<table style="border-spacing:2px">
	<tr></tr>
	<td width="280px">
			<img src="./files/award/mcm.png" width="200px" style="box-shadow: 4px 4px 8px #888; margin-left: 32px; margin-top: 0px;">
	</td>				
	<td style="font-size: 15px;">
		<b>2021, The Mathematical Contest in Modeling (MCM) </b>
		<p><b>美国大学生数学建模竞赛</b></p>
		<p><i style="color: red; display: inline;"><b>Meritorious Winner</b></i>🏅</p>
		<div style="font-size: 15px">With Ning Ji, Ziyue Zhang. Supervised by <a href="https://web.xidian.edu.cn/slzhang/">A/Prof. Shengli Zhang</a></div>
	</td>
</table>


<h2>Highlight Moments</h2>
  <div style="width: 100%;  background-color: white;">
	<!-- ; border: 5px solid red; -->
	<div style="width: 100%; overflow-x: scroll; white-space: nowrap;">
	  <!-- ###### -->
	  	<img src="./files/photo/highlight/01.jpg" style="width: auto; height: 160px;">
		<img src="./files/photo/highlight/02.jpg" style="width: auto; height: 160px;">
		<img src="./files/photo/highlight/03.jpg" style="width: auto; height: 160px;">
		<img src="./files/photo/highlight/04.jpg" style="width: auto; height: 160px;">
		<!-- <img src="./files/highlight/icpc.jpg" style="width: auto; height: 120px;"> -->
		<img src="./files/photo/highlight/05.jpg" style="width: auto; height: 160px;">
		<img src="./files/photo/highlight/06.jpg" style="width: auto; height: 160px;">
		<img src="./files/photo/highlight/07.jpg" style="width: auto; height: 160px;">
	</div>
  </div>
<div style="margin-top: 5px; font-size: small; margin-bottom: 0px;">⬆ scrollable</div>

</div>
<div id="footer">
	<div id="footer-text"></div>
</div>
	<p><center><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
		<!-- <span id="busuanzi_container_site_pv"><span id="busuanzi_value_site_pv" style="color: red;"></span> Total Pageviews, since Apr.2024</center></p> -->
		<a style="color: black; font-size: small;"><i>Latest updated in Jan. 2025</i></a>
	<p></p><center> This homepage code is derived from: <a href="https://tianxingchen.github.io/" target="_blank">Tianxing Chen</a> </center><p></p>


</div>


<div class="jvectormap-tip"></div></body><grammarly-desktop-integration data-grammarly-shadow-root="true"><template shadowrootmode="open"><style>
      div.grammarly-desktop-integration {
        position: absolute;
        width: 1px;
        height: 1px;
        padding: 0;
        margin: -1px;
        overflow: hidden;
        clip: rect(0, 0, 0, 0);
        white-space: nowrap;
        border: 0;
        -moz-user-select: none;
        -webkit-user-select: none;
        -ms-user-select:none;
        user-select:none;
      }

      div.grammarly-desktop-integration:before {
        content: attr(data-content);
      }
    </style><div aria-label="grammarly-integration" role="group" tabindex="-1" class="grammarly-desktop-integration" data-content="{&quot;mode&quot;:&quot;full&quot;,&quot;isActive&quot;:true,&quot;isUserDisabled&quot;:false}"></div></template></grammarly-desktop-integration></html>